{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './traffic-signs-data/train.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c8aaee38fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtesting_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./traffic-signs-data/test.p'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './traffic-signs-data/train.p'"
     ]
    }
   ],
   "source": [
    "#Traffic-Sign-Classifyer \n",
    "\n",
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"./traffic-signs-data/train.p\"\n",
    "validation_file= './traffic-signs-data/valid.p'\n",
    "testing_file = './traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "\n",
    "#Number of training examples\n",
    "n_train = y_train.shape[0]\n",
    "\n",
    "#Number of validation examples\n",
    "n_validation = y_valid.shape[0]\n",
    "\n",
    "#Number of testing examples.\n",
    "n_test = y_test.shape[0]\n",
    "\n",
    "#Traffic Sign Image shape\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "#Number of unique classes\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "\n",
    "#List of traffic sign labels names\n",
    "with open('signnames.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader, infile)\n",
    "    traffic_sign_labels = [rows[1] for rows in reader]\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)\n",
    "\n",
    "\n",
    "# Visualizations will be shown in the notebook.\n",
    "% matplotlib inline\n",
    "\n",
    "#Determine count of each label in splits\n",
    "train_set = Counter(y_train)\n",
    "valid_set = Counter(y_valid)\n",
    "test_set = Counter(y_test)\n",
    "\n",
    "print(\"Let's plot 10 sequential images starting randomly somewhere in the training split..\\n\")\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "n_images = 10\n",
    "image = X_train[index:index+n_images]\n",
    "image_labels = y_train[index:index+n_images]\n",
    "plt.figure(figsize=(15,5))\n",
    "for i in range(n_images):\n",
    "    plt.subplot(1,n_images,i+1)\n",
    "    plt.imshow(image[i])\n",
    "    plt.title(y_train[index+i])\n",
    "    #print('Image Labels:', y_train[index+i], \"-\", traffic_sign_labels[y_train[index+i]])\n",
    "#plt.savefig('.\\\\graphics\\\\label-images.png')\n",
    "plt.show()\n",
    "\n",
    "#Plot histogram of labels in each set\n",
    "print(\"Now, let's visualize the reprentation of each label in all three splits\")\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Data Distribution')\n",
    "train_bar = plt.bar(range(len(train_set)), train_set.values(), color = 'g')\n",
    "valid_bar = plt.bar(range(len(valid_set)), valid_set.values(), color = 'y', bottom = train_set.values())\n",
    "test_bar = plt.bar(range(len(test_set)), test_set.values(), color = 'r', bottom = (train_set + valid_set).values())\n",
    "\n",
    "plt.xticks(range(len(train_set)), train_set.keys())\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count of Label Occurence')\n",
    "plt.legend([train_bar, valid_bar, test_bar], ['Training Data', 'Validation Data', 'Test Data'])\n",
    "#plt.savefig('.\\\\graphics\\\\datahistogram.png')\n",
    "plt.show()\n",
    "\n",
    "n_threshold = 1500\n",
    "print(\"Let's quantify which labels need to be augmented by setting the threshold to be {:d} training samples\\n\"\n",
    "      .format(n_threshold))\n",
    "#Identify which labels need to be augmented\n",
    "labels_to_augment = []\n",
    "for key in train_set.keys():\n",
    "    if(train_set[key] < n_threshold):\n",
    "        labels_to_augment.append(key)\n",
    "print(\"The following {:d} labels have less than {:d} training samples and need to be augmented:\\n\"\n",
    "      .format(len(labels_to_augment), n_threshold))\n",
    "print(labels_to_augment)\n",
    "\n",
    "#Plot an image given a label\n",
    "label2plot = 38\n",
    "images2plot = X_train[np.where(y_train == label2plot)]\n",
    "random_index = np.random.randint(0, images2plot.shape[0])\n",
    "print('Image Label:', label2plot, \"-\", traffic_sign_labels[label2plot])\n",
    "fig = plt.figure(figsize=(2,2))\n",
    "fig = plt.imshow(images2plot[random_index])\n",
    "#plt.savefig('.\\\\graphics\\\\y-flipped-img.png')\n",
    "\n",
    "#Declare pre-processing functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Covert RGB image to grayscale\n",
    "def grayscale(img_set):\n",
    "    gray_img_set = []\n",
    "    for img in img_set:\n",
    "            gray_img_set.append(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY))\n",
    "    return gray_img_set\n",
    "\n",
    "#Perform histogram equalization using basic algorithm on grayscale images\n",
    "def equalize(img_set):\n",
    "    equal_img_set = []\n",
    "    for img in img_set:\n",
    "        equal_img_set.append(np.expand_dims(cv2.equalizeHist(img), axis = 2))\n",
    "    return equal_img_set\n",
    "\n",
    "#Perform histogram equalization using CLAHE algorithm on grayscale images\n",
    "def clahe_equalize(img_set):\n",
    "    clahe_img_set = []\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4,4))\n",
    "    for img in img_set:\n",
    "        clahe_img_set.append(np.expand_dims(clahe.apply(img), axis = 2))\n",
    "    return clahe_img_set\n",
    "\n",
    "#Perform histogram equalization using CLAHE algorithm on RGB->YUV->RGB images\n",
    "def clahe_equalize_RGB(img_set):\n",
    "    clahe_RGB_img_set = []\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4,4))\n",
    "    for img in img_set:\n",
    "        y, u, v = cv2.split(cv2.cvtColor(img, cv2.COLOR_RGB2YUV))\n",
    "        y = clahe.apply(y)\n",
    "        img = cv2.merge((y,u,v))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_YUV2RGB)\n",
    "        clahe_RGB_img_set.append(img)\n",
    "    return clahe_RGB_img_set\n",
    "\n",
    "#Merge RGB and gray channels to create image set of depth 4\n",
    "def merge_channels(img_gray_set, img_RGB_set):\n",
    "    merge_set = []\n",
    "    assert(len(img_RGB_set) == len(img_gray_set))\n",
    "    for img1, img2 in zip(img_gray_set, img_RGB_set):\n",
    "        img = cv2.merge((img1, img2))\n",
    "        merge_set.append(img)\n",
    "    return merge_set\n",
    "\n",
    "#Normalize image set using mean/std of entire image set\n",
    "def normalize(img_set, mean, std):\n",
    "    norm_img_set = []\n",
    "    for img in tqdm(img_set, total = len(img_set)):\n",
    "        norm_img_set.append((img - mean)/std)\n",
    "\n",
    "    return norm_img_set\n",
    "\n",
    "#Declare augmentation functions\n",
    "\n",
    "#References: \n",
    "#https://github.com/vxy10/ImageAugmentation \n",
    "#https://medium.com/@vivek.yadav/improved-performance-of-deep-learning-neural-network-models-on-traffic-sign-classification\n",
    "#        -using-6355346da2dc\n",
    "#https://navoshta.com/traffic-signs-classification/\n",
    "#https://github.com/vxy10/ImageAugmentation\n",
    "\n",
    "\n",
    "def transform_image(img, rt_range, xlate_range):\n",
    "    #Define Transformations - rotation and translation\n",
    "    pixels_x, pixels_y, channels = img.shape    \n",
    "    rt_angle = np.random.uniform(rt_range) - rt_range/2\n",
    "    xlate_x = np.random.uniform(xlate_range) - xlate_range/2\n",
    "    xlate_y = np.random.uniform(xlate_range) - xlate_range/2\n",
    "    \n",
    "    M_rot = cv2.getRotationMatrix2D((pixels_y/2, pixels_x/2), rt_angle, 1)\n",
    "    M_xlate = np.float32([[1,0,xlate_x],[0,1,xlate_y]])\n",
    "    \n",
    "    #Transform image\n",
    "    img = cv2.warpAffine(img, M_xlate, (pixels_y, pixels_x))\n",
    "    img = cv2.warpAffine(img, M_rot, (pixels_y, pixels_x))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def augment_set(X_data, y_data, labels, threshold, rt_range, xlate_range):\n",
    "    \n",
    "    #Declare all variables\n",
    "    images_dict = {}\n",
    "    n_imgs = 0 #total # of images that will be perturbed\n",
    "    pixels_x, pixels_y, channels = X_data.shape[1:4]\n",
    "    n_aug_imgs = {} #holds # of images that need to be created for each label\n",
    "    \n",
    "    #Determine indices of images that need to be augmented and their corresponding labels\n",
    "    for label in labels:\n",
    "        images_dict[label] = X_data[np.where(y_data == label)]\n",
    "        n_aug_imgs[label] =  threshold - images_dict[label].shape[0]\n",
    "        n_imgs += images_dict[label].shape[0]\n",
    "        \n",
    "    #Determine how many perturbed images need to be created for each label to meet the threshold\n",
    "    print (\"# of images available to be perturbed:\", n_imgs)\n",
    "    print(\"# of images to be created per label\", n_aug_imgs)\n",
    "    print(\"Total # of images to be created:\", sum(n_aug_imgs.values()))\n",
    "    \n",
    "    #Create empty array to hold perturbed images\n",
    "    new_images = np.empty([sum(n_aug_imgs.values()), pixels_x, pixels_y, channels], dtype = np.uint8)\n",
    "    new_labels = np.empty([sum(n_aug_imgs.values())], dtype = int)\n",
    "    #print(\"The shapes of the new arrays are:\", new_images.shape, new_labels.shape)\n",
    "    \n",
    "    #Loop through labels/image arrays, determine # of perturbations for each image in label then perturb image\n",
    "    offset = 0\n",
    "    for label, img_array in tqdm(images_dict.items(), total = len(labels)):\n",
    "        #print(\"The image counter has been reset, starting label\", label)\n",
    "        new_img_count = 0\n",
    "        n_pertubations = math.ceil(n_aug_imgs[label]/images_dict[label].shape[0])\n",
    "        #print(\"Labels {:d} has {:d} images and {:d} new images need to be created\"\n",
    "              #.format(label, images_dict[label].shape[0], n_aug_imgs[label]))\n",
    "        for img_index, img in enumerate(img_array):\n",
    "            #print(\"We will create {:d} pertubations for this image.\".format(n_pertubations))\n",
    "                \n",
    "            #create the desired number of pertubations\n",
    "            for i in range(n_pertubations):\n",
    "                #Create the new image and label\n",
    "                new_images[i+img_index*n_pertubations+offset] = transform_image(img, rt_range, xlate_range)\n",
    "                #print(\"The new label is\", label, \"at index\", i+img_index*n_pertubations+offset)\n",
    "                new_labels[i+img_index*n_pertubations+offset] = label\n",
    "                new_img_count += 1\n",
    "                \n",
    "                if (new_img_count == n_aug_imgs[label]):\n",
    "                    break\n",
    "            \n",
    "            if (new_img_count == n_aug_imgs[label]):\n",
    "                break\n",
    "        \n",
    "        offset += n_aug_imgs[label]\n",
    "        #print(\"{:d} new images created for label {:d}\".format(new_img_count, label))\n",
    "        #print(\"The offset is now {:d}\\n\".format(offset))\n",
    "        \n",
    "        aug_X_data = np.concatenate((X_data, new_images), 0)\n",
    "        aug_y_data = np.concatenate((y_data, new_labels), 0)\n",
    "\n",
    "\n",
    "    return (aug_X_data, aug_y_data) \n",
    "\n",
    "#Do image mirroring and rotations on labels that are invariant to the operation\n",
    "def basic_augment(X_data, y_data, xflip, yflip, rot120):    \n",
    "   \n",
    "    #Define variables and dictionaries\n",
    "    xflip_dict = {}\n",
    "    yflip_dict = {}\n",
    "    rot_dict = {}\n",
    "    n_xflip_imgs, n_yflip_imgs, n_rot_imgs = 0, 0, 0\n",
    "    \n",
    "    #Setup up rotation matrices\n",
    "    pixels_x, pixels_y, channels = X_data.shape[1:4]\n",
    "    M1 = cv2.getRotationMatrix2D((pixels_y/2,pixels_x/2), 120, 1)\n",
    "    M2 = cv2.getRotationMatrix2D((pixels_y/2,pixels_x/2), 240, 1)\n",
    "    \n",
    "    #Populate dictonary with images to be operated on with key = label\n",
    "    for old_label, new_label in xflip:\n",
    "        xflip_dict[old_label] = X_data[np.where(y_data == old_label)]\n",
    "        n_xflip_imgs += xflip_dict[old_label].shape[0]\n",
    "    \n",
    "    for old_label, new_label in yflip:\n",
    "        yflip_dict[old_label] = X_data[np.where(y_data == old_label)]\n",
    "        n_yflip_imgs += yflip_dict[old_label].shape[0]\n",
    "  \n",
    "    for old_label, new_label in rot120:\n",
    "        rot_dict[old_label] = X_data[np.where(y_data == old_label)]\n",
    "        n_rot_imgs += rot_dict[old_label].shape[0]\n",
    "    \n",
    "    #Create empty arrays to hold new images and their corresponding labels\n",
    "    xflip_images = np.empty([n_xflip_imgs, pixels_x, pixels_y, channels], dtype = np.uint8)\n",
    "    xflip_labels = np.empty([n_xflip_imgs], dtype = int)\n",
    "    yflip_images = np.empty([n_yflip_imgs, pixels_x, pixels_y, channels], dtype = np.uint8)\n",
    "    yflip_labels = np.empty([n_yflip_imgs], dtype = int)\n",
    "    rot_images = np.empty([2*n_rot_imgs, pixels_x, pixels_y, channels], dtype = np.uint8)\n",
    "    rot_labels = np.empty([2*n_rot_imgs], dtype = int)\n",
    "\n",
    "    #Perform xflips, yflips and 120deg, 240deg rotations\n",
    "    offset = 0\n",
    "    for label, img_array in xflip_dict.items():\n",
    "        if img_array.shape[0] != 0:\n",
    "            for index, img in enumerate(img_array):\n",
    "                xflip_images[index+offset] = cv2.flip(img,0)\n",
    "                xflip_labels[index+offset] = label\n",
    "            offset += img_array.shape[0]\n",
    "    \n",
    "    offset = 0\n",
    "    for label, img_array in yflip_dict.items():\n",
    "        if img_array.shape[0] != 0:\n",
    "            for index, img in enumerate(img_array):\n",
    "                yflip_images[index+offset] = cv2.flip(img,1)\n",
    "                yflip_labels[index+offset] = yflip[np.where(yflip[:,0] == label),1][0][0]\n",
    "            offset += img_array.shape[0]\n",
    "    \n",
    "    offset = 0\n",
    "    for label, img_array in rot_dict.items():\n",
    "        if img_array.shape[0] != 0:\n",
    "            for index, img in enumerate(img_array):\n",
    "                rot_images[index+offset] = cv2.warpAffine(img, M1, (pixels_y,pixels_x))\n",
    "                rot_images[index+n_rot_imgs+offset] = cv2.warpAffine(img, M2, (pixels_y,pixels_x))\n",
    "                rot_labels[index+offset] = label\n",
    "                rot_labels[index+n_rot_imgs+offset] = label\n",
    "            offset += img_array.shape[0]\n",
    "    \n",
    "    #Append arrays and return new training data set\n",
    "    \n",
    "    aug_X_data = np.concatenate((X_data, xflip_images, yflip_images, rot_images), 0)\n",
    "    aug_y_data = np.concatenate((y_data, xflip_labels, yflip_labels, rot_labels), 0)\n",
    "    print(\"Number of x-flipped images:\", xflip_images.shape[0])\n",
    "    print(\"Number of y-flipped images:\", yflip_images.shape[0])\n",
    "    print(\"Number of rotated images (2x the input):\", rot_images.shape[0])\n",
    "    print(\"Total # of appended images:\", xflip_images.shape[0] + yflip_images.shape[0] + rot_images.shape[0])\n",
    "    print(\"Original training data set had {:d} images\".format(X_data.shape[0]))\n",
    "    print(\"Augmented training data set has {:d} images\\n\".format(aug_X_data.shape[0]))\n",
    "        \n",
    "    return (aug_X_data, aug_y_data)\n",
    "\n",
    "    #Do Preliminary Augmentation on Dataset (i.e. image mirroring and simple rotations)\n",
    "\n",
    "#Identify which labels can be flipped horizontally/vertically and determine the new label if changed [oldlabel, new label].\n",
    "#This is not an exhaustive list since not all labels that can be flipped require augmentation\n",
    "\n",
    "labels_x_flip = np.array([[17,17]])\n",
    "labels_y_flip = np.array([[19,20], [20,19], [22,22], [26,26], [30,30], [33,34], [34,33], [36,37], [37,36], [38,39]])\n",
    "labels_rot_120 = np.array([[40,40]])\n",
    "\n",
    "X_train_aug, y_train_aug = basic_augment(X_train, y_train, labels_x_flip, labels_y_flip, labels_rot_120)\n",
    "\n",
    "print(\"Lets visualize the new augmented data set. Some of the lables have been augmented but\" \n",
    "      \" clearly, more augmentation is required:\")\n",
    "\n",
    "#Plot histogram of labels in each set\n",
    "aug_train_set = Counter(y_train_aug)\n",
    "valid_set = Counter(y_valid)\n",
    "test_set = Counter(y_test)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title(' Augmented Data Distribution')\n",
    "aug_train_bar = plt.bar(range(len(aug_train_set)), aug_train_set.values(), color = 'g')\n",
    "valid_bar = plt.bar(range(len(valid_set)), valid_set.values(), color = 'y', bottom = aug_train_set.values())\n",
    "test_bar = plt.bar(range(len(test_set)), test_set.values(), color = 'r', bottom = (aug_train_set + valid_set).values())\n",
    "\n",
    "plt.xticks(range(len(aug_train_set)), aug_train_set.keys())\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count of Label Occurence')\n",
    "plt.legend([aug_train_bar, valid_bar, test_bar], ['Augmented Training Data', 'Validation Data', 'Test Data'])\n",
    "#plt.savefig('.\\\\graphics\\\\AugDataHistogram.png')\n",
    "plt.show()\n",
    "\n",
    "#Identify which labels need to be augmented\n",
    "labels_to_augment = []\n",
    "n_threshold = 1500\n",
    "for key in aug_train_set.keys():\n",
    "    if(aug_train_set[key] < n_threshold):\n",
    "        labels_to_augment.append(key)\n",
    "print(\"The following {:d} labels have less than {:d} training samples and need to be augmented:\\n\"\n",
    "      .format(len(labels_to_augment), n_threshold))\n",
    "print(labels_to_augment)\n",
    "\n",
    "#Perform additional augmentation to get under-represented labels in the training set up to the threshold\n",
    "rt_range = 15\n",
    "xlate_range = 5\n",
    "X_train_aug2, y_train_aug2 = augment_set(X_train_aug, y_train_aug, labels_to_augment, \n",
    "                                         n_threshold, rt_range, xlate_range)\n",
    "\n",
    "print(\"The final augmented training set has {:d} images\".format(X_train_aug2.shape[0]))\n",
    "\n",
    "print(\"Let's plot that histogram again...\")\n",
    "\n",
    "aug_train_set = Counter(y_train_aug2)\n",
    "valid_set = Counter(y_valid)\n",
    "test_set = Counter(y_test)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title(' Augmented Data Distribution')\n",
    "aug_train_bar = plt.bar(range(len(aug_train_set)), aug_train_set.values(), color = 'g')\n",
    "valid_bar = plt.bar(range(len(valid_set)), valid_set.values(), color = 'y', bottom = aug_train_set.values())\n",
    "test_bar = plt.bar(range(len(test_set)), test_set.values(), color = 'r', bottom = (aug_train_set + valid_set).values())\n",
    "\n",
    "plt.xticks(range(len(aug_train_set)), aug_train_set.keys())\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count of Label Occurence')\n",
    "plt.legend([aug_train_bar, valid_bar, test_bar], ['Augmented Training Data', 'Validation Data', 'Test Data'])\n",
    "#plt.savefig('.\\\\graphics\\\\FinalDataHistogram.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"The data distribution is much more balanced. Let's take a look at some of the newly transformed images:\")\n",
    "index = random.randint(34799, len(X_train_aug2))\n",
    "n_images = 10\n",
    "image = X_train_aug2[index:index+n_images]\n",
    "image_labels = y_train_aug2[index:index+n_images]\n",
    "plt.figure(figsize=(15,3))\n",
    "for i in range(n_images):\n",
    "    plt.subplot(1,n_images,i+1)\n",
    "    plt.imshow(image[i])\n",
    "plt.savefig('.\\\\graphics\\\\augmentation.png')\n",
    "plt.show()\n",
    "\n",
    "#Call Pre-Processing functions\n",
    "\n",
    "#Create equalized grayscale images\n",
    "X_train_gray = grayscale(X_train_aug2)\n",
    "X_valid_gray = grayscale(X_valid)\n",
    "X_test_gray = grayscale(X_test)\n",
    "\n",
    "X_train_clahe = clahe_equalize(X_train_gray)\n",
    "X_valid_clahe = clahe_equalize(X_valid_gray)\n",
    "X_test_clahe = clahe_equalize(X_test_gray)\n",
    "\n",
    "#Create equalized RGB images\n",
    "X_train_clahe_RGB = clahe_equalize_RGB(X_train_aug2)\n",
    "X_valid_clahe_RGB = clahe_equalize_RGB(X_valid)\n",
    "X_test_clahe_RGB = clahe_equalize_RGB(X_test)\n",
    "\n",
    "#Merge images\n",
    "X_train_merge = merge_channels(X_train_clahe, X_train_clahe_RGB)\n",
    "X_valid_merge = merge_channels(X_valid_clahe, X_valid_clahe_RGB)\n",
    "X_test_merge = merge_channels(X_test_clahe, X_test_clahe_RGB)\n",
    "\n",
    "#Compute mean and standard for training set and apply to all splits\n",
    "#Ref: http://cs231n.github.io/neural-networks-2/\n",
    "#mean = np.mean(X_train_clahe)\n",
    "#std = np.std(X_train_clahe)\n",
    "\n",
    "mean = np.mean(X_train_merge)\n",
    "std = np.std(X_train_merge)\n",
    "\n",
    "#Normalize images in each split\n",
    "X_train_processed = normalize(X_train_merge, mean, std)\n",
    "X_valid_processed = normalize(X_valid_merge, mean, std)\n",
    "X_test_processed = normalize(X_test_merge, mean, std)\n",
    "\n",
    "y_train_processed = y_train_aug2\n",
    "\n",
    "#Check Pre-Processing\n",
    "#index = random.randint(34799, len(X_train_aug2))\n",
    "index = 35242\n",
    "image = X_train_aug2[index]\n",
    "print(\"Index:\", index)\n",
    "image_gray = X_train_gray[index]\n",
    "image_processed = X_train_clahe[index]\n",
    "RGB_enhanced = X_train_clahe_RGB[index]\n",
    "\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.subplot(1,4,1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(image)\n",
    "plt.subplot(1,4,2)\n",
    "plt.title('Grayscale Image')\n",
    "plt.imshow(image_gray.squeeze(), cmap = 'gray')\n",
    "plt.subplot(1,4,3)\n",
    "plt.title('CLAHE Image')\n",
    "plt.imshow(image_processed.squeeze(), cmap = 'gray')\n",
    "plt.subplot(1,4,4)\n",
    "plt.title('RGB CLAHE Image')\n",
    "plt.imshow(RGB_enhanced)\n",
    "plt.savefig('.\\\\graphics\\\\image-processing.png')\n",
    "print('Image Label:', y_train_aug2[index], \"-\", traffic_sign_labels[y_train_aug2[index]])\n",
    "\n",
    "#Import all libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "\n",
    "#Implement LeNet-5 Architecture - Use LeNet Lab as starting point\n",
    "\n",
    "def LeNet5(x):\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    #Define Weights & Biases - change depth of wc1 and wc2 from 1,6 - 4,10, 6,16 - 10,20, wd1 from 400 to 500\n",
    "    weights = {\n",
    "                'wc1':tf.Variable(tf.truncated_normal([5,5,4,10], mu, sigma)),\n",
    "                'wc2':tf.Variable(tf.truncated_normal([5,5,10,20], mu, sigma)),\n",
    "                'wd1':tf.Variable(tf.truncated_normal([500,120], mu, sigma)),\n",
    "                'wd2':tf.Variable(tf.truncated_normal([120,84], mu, sigma)),\n",
    "                'wd3':tf.Variable(tf.truncated_normal([84,43], mu, sigma))\n",
    "    }\n",
    "    #change bc1 and bc2 from 6,16 to 10,20\n",
    "    bias = {\n",
    "                'bc1':tf.Variable(tf.truncated_normal([10], mu, sigma)), \n",
    "                'bc2':tf.Variable(tf.truncated_normal([20], mu, sigma)),\n",
    "                'bd1':tf.Variable(tf.truncated_normal([120], mu, sigma)),\n",
    "                'bd2':tf.Variable(tf.truncated_normal([84], mu, sigma)),\n",
    "                'bd3':tf.Variable(tf.truncated_normal([43], mu, sigma))\n",
    "    }\n",
    "    \n",
    "    #Include L2 regularization \n",
    "    L2_norm = tf.nn.l2_loss(weights['wc1']) + tf.nn.l2_loss(weights['wc2']) + \\\n",
    "                tf.nn.l2_loss(weights['wd1']) + tf.nn.l2_loss(weights['wd2']) + tf.nn.l2_loss(weights['wd3'])\n",
    "    \n",
    "    stride_conv = [1,1,1,1]\n",
    "    stride_pool = [1,2,2,1]\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6 x10.\n",
    "    conv1 = tf.nn.conv2d(x, weights['wc1'], strides = stride_conv, padding = 'VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, bias['bc1'])\n",
    "\n",
    "    # Activation. - Use ReLU\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6 x10.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize = stride_pool, strides = stride_pool, padding = 'VALID')\n",
    "    \n",
    "    # Layer 2: Convolutional. Output = 10x10x16 x20.\n",
    "    conv2 = tf.nn.conv2d(conv1, weights['wc2'], strides = stride_conv, padding = 'VALID')\n",
    "    conv2 = tf.nn.bias_add(conv2, bias['bc2'])\n",
    "    \n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "    # Pooling. Input = 10x10x16 x20. Output = 5x5x16 x20.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize = stride_pool, strides = stride_pool, padding = 'VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400 to 500.\n",
    "    fc1 = flatten(conv2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 400 to 500. Output = 120.\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), bias['bd1'])\n",
    "    \n",
    "    # Activation.\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2 = tf.add(tf.matmul(fc1, weights['wd2']), bias['bd2'])\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    logits = tf.add(tf.matmul(fc2, weights['wd3']), bias['bd3'])\n",
    "    \n",
    "    return logits, L2_norm, conv1, conv2, fc1, fc2\n",
    "\n",
    "\n",
    "#Define Variables for model training - change x from depth 1 to 4\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 4))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "\n",
    "#Define hyperparameters\n",
    "RATE = 0.0005\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "beta = 0.001\n",
    "\n",
    "#Define Training Pipeline\n",
    "logits, L2_norm, conv1, conv2, fc1, fc2 = LeNet5(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = RATE)\n",
    "training_operation = optimizer.minimize(loss_operation + beta*L2_norm)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#Accuracy & loss evaluation function\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    n_batches = math.ceil(num_examples/BATCH_SIZE)\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "    sess = tf.get_default_session()\n",
    "    print(\"Validating & calculating loss...\")\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        loss, accuracy = sess.run([loss_operation, accuracy_operation], feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        total_loss += (loss * len(batch_x))\n",
    "    return (total_loss / num_examples, total_accuracy / num_examples)\n",
    "\n",
    "#Create Session & conduct training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train_processed)\n",
    "    n_batches = math.ceil(num_examples/BATCH_SIZE)\n",
    "    \n",
    "    train_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    print(\"Training Model...\")\n",
    "    for i in tqdm(range(EPOCHS), total = EPOCHS):\n",
    "        X_train_processed, y_train_processed = shuffle(X_train_processed, y_train_processed)\n",
    "        print(\"Training on batches...\")\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train_processed[offset:end], y_train_processed[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "        \n",
    "        #Calculate losses & accuracies\n",
    "        training_loss, training_accuracy = evaluate(X_train_processed, y_train_processed)\n",
    "        validation_loss, validation_accuracy = evaluate(X_valid_processed, y_valid)\n",
    "        train_accuracy.append(training_accuracy*100)\n",
    "        valid_accuracy.append(validation_accuracy*100)\n",
    "        train_loss.append(training_loss)\n",
    "        valid_loss.append(validation_loss)\n",
    "        \n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Training Accuracy = {:.3f}\".format(training_accuracy))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './LeNet5_Run3')\n",
    "    print(\"Training Completed!\")\n",
    "\n",
    "\n",
    "#Plot Validation accuracy\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,EPOCHS+1), train_accuracy, '-o', label = 'Training Set')\n",
    "plt.plot(range(1,EPOCHS+1), valid_accuracy, '-x', label = \"Validation Set\")\n",
    "plt.plot(range(1,EPOCHS+1), [93]*EPOCHS, '--k', label = '93% Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy [%]')\n",
    "plt.xlim(1, EPOCHS+1)\n",
    "plt.ylim(50,100)\n",
    "plt.xticks(np.arange(1, EPOCHS+1, 1))\n",
    "plt.yticks(np.arange(50,105,5))\n",
    "plt.title('Accuracy Trend')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(1, EPOCHS+1), train_loss, '-o', label = 'Training Set')\n",
    "plt.plot(range(1, EPOCHS+1), valid_loss, '-x', label = 'Validation Set')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Function')\n",
    "plt.xlim(1, EPOCHS+1)\n",
    "plt.xticks(np.arange(1, EPOCHS+1, 1))\n",
    "plt.title('Loss Trend')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './LeNet5_Run3')\n",
    "    test_loss, test_accuracy = evaluate(X_test_processed, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "\n",
    "\n",
    "#Calculate predictions on the test split\n",
    "test_soft_max = tf.nn.softmax(logits)\n",
    "top1 = tf.nn.top_k(test_soft_max, 1)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './LeNet5_Run3')\n",
    "    test_split_predictions = sess.run(top1, feed_dict={x: X_test_processed, y: y_test, keep_prob: 1})\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "test_predictions = test_split_predictions[1]\n",
    "\n",
    "#Let's calculate precision & recall\n",
    "precision = metrics.precision_score(y_test, test_predictions, average=None)\n",
    "recall = metrics.recall_score(y_test, test_predictions, average=None)\n",
    "\n",
    "print(\"Traffic Sign Label                                  # of samples Precision    Recall\\n\")\n",
    "for i in range(len(traffic_sign_labels)):\n",
    "    print(\"{0:<50}: {1:8d} {2:10.2f}% {3:10.2f}%\".format(traffic_sign_labels[i],test_set[i], precision[i]*100, recall[i]*100))\n",
    "\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "#Import images\n",
    "test_images = np.empty([13,32,32,3], dtype = np.uint8)\n",
    "test_labels = np.empty([13], dtype = int)\n",
    "\n",
    "for i, filename in enumerate(glob('.\\\\test_images\\*.jpg')):\n",
    "    image = mpimg.imread(filename)\n",
    "    image = cv2.resize(image, (32,32), interpolation=cv2.INTER_AREA)\n",
    "    test_images[i] = image\n",
    "    test_labels[i] = re.findall(r'\\d+', filename)[1]\n",
    "\n",
    "print(\"The test images array is:\", test_images.shape)\n",
    "print(\"The test labels array is:\", test_labels.shape)\n",
    "print(\"Let's take a look at all the resized images and their labels:\")\n",
    "\n",
    "plt.figure(figsize=(14,2))\n",
    "for i in range(test_images.shape[0]):\n",
    "    plt.subplot(1,test_images.shape[0],i+1)\n",
    "    plt.imshow(test_images[i])\n",
    "    plt.title(\"{:d}\".format(test_labels[i]))\n",
    "plt.show()\n",
    "\n",
    "#Process the images\n",
    "test_images_gray = grayscale(test_images)\n",
    "test_images_clahe = clahe_equalize(test_images_gray)\n",
    "test_images_clahe_RGB = clahe_equalize_RGB(test_images)\n",
    "test_images_merge = merge_channels(test_images_clahe, test_images_clahe_RGB)\n",
    "\n",
    "mean_test = np.mean(test_images_merge)\n",
    "std_test = np.std(test_images_merge)\n",
    "\n",
    "test_images_processed = normalize(test_images_merge, mean_test, std_test)\n",
    "\n",
    "\n",
    "#Calculate predictions\n",
    "soft_max = tf.nn.softmax(logits)\n",
    "top5 = tf.nn.top_k(soft_max, 5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './LeNet5_Run3')\n",
    "    predictions = sess.run(top5, feed_dict={x: test_images_processed, y: test_labels, keep_prob: 1})\n",
    "\n",
    "\n",
    "### Calculate the accuracy for these 5 new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './LeNet5_Run3')\n",
    "    test_images_loss, test_images_accuracy = evaluate(test_images_processed, test_labels)\n",
    "    print(\"Test Image Accuracy = {:.3f}\".format(test_images_accuracy))\n",
    "\n",
    "\n",
    "prob_predict, label_predict = predictions[0], predictions[1]\n",
    "\n",
    "print(\"Let's visualize the predictions instead of printing them.\")\n",
    "print(\"On the left is the probability of each top-5 label. The title of the barchart represents the prediction.\")\n",
    "print(\"On the right is the actual sign and label\")\n",
    "fig = plt.figure(figsize=(10,50))\n",
    "for i in range(13):\n",
    "    ax = plt.subplot(13,2,2*i+1)\n",
    "    plt.barh(range(5), prob_predict[i], height = 0.5)\n",
    "    ax.set_yticks(range(5))\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_yticklabels(label_predict[i])\n",
    "    plt.title(\"{}\".format(traffic_sign_labels[label_predict[i,0]]))\n",
    "    plt.subplot(13,2,2*i+2)\n",
    "    plt.imshow(test_images[i])\n",
    "    plt.title(traffic_sign_labels[test_labels[i]])\n",
    "plt.show()\n",
    "fig.savefig('softmax.png', bbox_inches='tight')\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input, keep_prob: 1})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")\n",
    "\n",
    "print(\"Let's take a look at an unprocessed image\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(test_images[0])\n",
    "plt.show()\n",
    "#Re-shape processed image to (1,32,32,4)\n",
    "image_input = np.expand_dims(test_images_processed[0], 0)\n",
    "\n",
    "\n",
    "#Visualize feature maps for 1st conv layer. Depth is 10.\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './LeNet5_Run3')\n",
    "    outputFeatureMap(image_input, conv1)\n",
    "\n",
    "#Visualize feature maps for 2nd conv layer. Depth is 20.\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './LeNet5_Run3')\n",
    "    outputFeatureMap(image_input, conv2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
